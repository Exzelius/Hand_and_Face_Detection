{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m python\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vision\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# STEP 1: Import the necessary modules.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m python\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vision\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "\n",
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an HandLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options,\n",
    "                                       num_hands=2)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"image.avif\")\n",
    "\n",
    "# STEP 4: Detect hand landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the classification result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "cv2_imshow(cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Create a MediaPipe HandLandmarker detector. \n",
    "# Requires MediaPipe 0.9.1 and above.\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=1)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\n",
    "\n",
    "# Define the 3D model landmarks (relative to the hand center)\n",
    "model_landmarks_list = [\n",
    "    # These values are based on a 3D model of a hand with 21 key landmarks.\n",
    "    # You should adjust these to match a hand model or retrieve from a hand 3D model.\n",
    "    np.array([\n",
    "        [0, 0, 0],  # Example: palm center (index 0)\n",
    "        [1, 0, 0],  # Example: thumb base (index 1)\n",
    "        # Add other landmarks for all 21 hand points...\n",
    "    ])\n",
    "]\n",
    "\n",
    "def predict(frame):\n",
    "    \"\"\"\n",
    "    ---------------------------------------\n",
    "    TODO: Task 1.\n",
    "    Implement the hand landmark prediction.\n",
    "    ---------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform hand landmark detection\n",
    "    detection_result = detector.detect(frame)\n",
    "\n",
    "    # Check if any hands were detected\n",
    "    if detection_result and detection_result.hand_landmarks:\n",
    "        landmarks = []\n",
    "        for hand_landmarks in detection_result.hand_landmarks:\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                # Append the x, y, z coordinates of each landmark\n",
    "                landmarks.append((landmark.x, landmark.y, landmark.z))\n",
    "        return np.array(landmarks)\n",
    "    else:\n",
    "        return None\n",
    "    # return detection_result\n",
    "\n",
    "def draw_landmarks_on_image(image, detection_result):\n",
    "    \"\"\"\n",
    "    A helper function to draw the detected 2D landmarks on an image \n",
    "    \"\"\"\n",
    "    if not detection_result:\n",
    "        return image \n",
    "    \n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    # Loop through the detected hands and draw directly on the image\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark. z) for landmark in hand_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style())\n",
    "    return image\n",
    "\n",
    "def get_camera_matrix(frame_width, frame_height, scale=1.0): \n",
    "    \"\"\"\n",
    "    The camera matrix is a matrix of size 3x3 that captures the intrinsic properties of the camera including focal length and center of projection. \n",
    "    One can project a 3D point in the camera space to the image plane by multiplying it with the intrinsic matrix. \n",
    "    \n",
    "    For example, let the 3D point by P = np.array([X, Y, Z]) (column vector). Let camera matrix be K. In numpy's code, the projected point is: \n",
    "    \n",
    "    p = K @ P \n",
    "    p[0] /= p[2]\n",
    "    p[1] /= p[2]\n",
    "    \n",
    "    Here the division by p[2] is the perspective division. After division, p[0] and p[1] are the x and y coordinate of the image pixel. \n",
    "    \"\"\"\n",
    "    \n",
    "    # As we do not know exactly the focal length, we estimate it by a scale of the image size. We can do camera calibration to find a more accurate focal length value but this is out of the scope of this assignment. \n",
    "    focal_length = frame_width * scale \n",
    "    \n",
    "    # Note this aspect ratio reflects ratio in the physical pixel size, almost 1, not the aspect ratio between image width and height as in OpenGL. \n",
    "    aspect_ratio = 1.0\n",
    "    \n",
    "    # Center of projection. We simply take the image center.\n",
    "    center = (frame_width / 2.0, frame_height / 2.0)\n",
    "    \n",
    "    # 3x3 intrinsic matrix\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]], dtype = \"double\"\n",
    "    )\n",
    "    return camera_matrix\n",
    "\n",
    "def get_fov_y(camera_matrix, frame_height):\n",
    "    \"\"\"\n",
    "    Compute the vertical field of view from focal length for OpenGL rendering\n",
    "    \"\"\"\n",
    "    focal_length_y = camera_matrix[1][1]\n",
    "    fov_y = np.rad2deg(2 * np.arctan2(frame_height, 2 * focal_length_y))\n",
    "    return fov_y\n",
    "\n",
    "def get_matrix44(rvec, tvec):\n",
    "\t\"\"\"\n",
    "\tConvert the rotation vector and translation vector to a 4x4 matrix\n",
    "\t\"\"\"\n",
    "\trvec = np.asarray(rvec)\n",
    "\ttvec = np.asarray(tvec)\n",
    "\tT = np.eye(4)\n",
    "\tR, jac = cv2.Rodrigues(rvec)\n",
    "\tT[:3, :3] = R\n",
    "\tT[:3, 3] = tvec\n",
    "\treturn T\n",
    "\n",
    "def solvepnp(model_landmarks_list, image_landmarks_list, \n",
    "            camera_matrix, frame_width, frame_height): \n",
    "    \"\"\"\n",
    "    Solve a global rotation and translation to bring the hand model points into the camera space, so that their projected points match the hands. \n",
    "    \n",
    "    Input: \n",
    "      model_landmarks_list: a list of 21x3 matrixes representing hand landmarks. The coordinates are relative to the hand center.\n",
    "      \n",
    "      image_landmarks_list: a list of 21x2 matrixes representing hand landmarks in image space, normalized to [0, 1]. \n",
    "      \n",
    "    Output: \n",
    "      world_landmarks_list: a list of 21x3 matrixes representing hand landmarks in absolute world space.\n",
    "    \"\"\"\n",
    "    if not model_landmarks_list:\n",
    "        return []\n",
    "    \n",
    "    world_landmarks_list = []\n",
    "    \n",
    "    for (model_landmarks, image_landmarks) in zip(model_landmarks_list, image_landmarks_list):\n",
    "        \n",
    "        # N x 3 matrix\n",
    "        model_points = np.float32([[l.x, l.y, l.z] for l in model_landmarks])\n",
    "        image_points = np.float32([[l.x * frame_width, l.y * frame_height] for l in image_landmarks])\n",
    "        \n",
    "        world_points = np.copy(model_points)\n",
    "        \n",
    "        \"\"\"\n",
    "        ----------------------------------------------------------------------\n",
    "        TODO: Task 2. \n",
    "        Call OpenCV's solvePnP function here.\n",
    "        ----------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        # # Solve for rotation and translation vectors using solvePnP\n",
    "        # success, rvec, tvec = cv2.solvePnP(model_points, image_points, camera_matrix, None)\n",
    "\n",
    "        # if success:\n",
    "        #     # Transform the model points to world coordinates\n",
    "        #     rot_matrix, _ = cv2.Rodrigues(rvec)\n",
    "        #     world_points = np.dot(model_points, rot_matrix.T) + tvec.T\n",
    "\n",
    "        # Store all 3D landmarks\n",
    "        world_landmarks_list.append(world_points)\n",
    "    \n",
    "    return world_landmarks_list\n",
    "\n",
    "def reproject(world_landmarks_list, image_landmarks_list, \n",
    "              camera_matrix, frame_width, frame_height): \n",
    "    \"\"\"\n",
    "    Perform a perspective projection of 3D points onto the image plane\n",
    "    and return the projected points.\n",
    "    \"\"\"\n",
    "    reprojection_points_list = []\n",
    "    reprojection_error = 0.0\n",
    "    for (world_landmarks, image_landmarks) in zip(world_landmarks_list, image_landmarks_list):\n",
    "        # Perspective projection by multiplying with the intrinsic matrix\n",
    "        output = world_landmarks.dot(camera_matrix.T)\n",
    "        \n",
    "        # Perspective division\n",
    "        output[:, 0] /= output[:, 2]\n",
    "        output[:, 1] /= output[:, 2]\n",
    "        \n",
    "        # Store the results into a list for visualization later\n",
    "        reprojection_points_list.append(output[:, :2])\n",
    "    \n",
    "        # Calculate the reprojection error, per point\n",
    "        image_points = np.float32([[l.x * frame_width, l.y * frame_height] for l in image_landmarks])\n",
    "        reprojection_error += np.linalg.norm(output[:, :2] - image_points) / len(output) / len(world_landmarks_list)\n",
    "    \n",
    "    return reprojection_error, reprojection_points_list\n",
    "\n",
    "\"\"\"\n",
    "This is an example main function that displays the video camera and the detection results in 2D landmarks with an OpenCV window.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Initializing current time and precious time for calculating the FPS\n",
    "    previousTime = 0\n",
    "    currentTime = 0\n",
    "\n",
    "    #  # Define the camera matrix (assuming some focal length and principal point)\n",
    "    # focal_length = 800  # Adjust as needed\n",
    "    # center = (capture.get(cv2.CAP_PROP_FRAME_WIDTH) / 2, capture.get(cv2.CAP_PROP_FRAME_HEIGHT) / 2)\n",
    "    # camera_matrix = np.array([[focal_length, 0, center[0]],\n",
    "    #                           [0, focal_length, center[1]],\n",
    "    #                           [0, 0, 1]], dtype=np.float32)\n",
    "    \n",
    "    while capture.isOpened():\n",
    "        # capture frame by frame\n",
    "        ret, frame = capture.read()\n",
    "    \n",
    "        # resizing the frame for better view\n",
    "        aspect_ratio = frame.shape[1] / frame.shape[0]\n",
    "        frame = cv2.resize(frame, (int(720 * aspect_ratio), 720))\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Converting the from BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "        # Making predictions\n",
    "        detection_result = predict(frame)\n",
    "    \n",
    "        # Visualize 2D landmarks\n",
    "        frame = draw_landmarks_on_image(frame, detection_result)\n",
    "        \n",
    "        \"\"\"\n",
    "        -------------------------------------------------------------------\n",
    "        TODO: Task 2. \n",
    "        SolvePnP, and visualize the reprojected landmarks. \n",
    "        The reprojected points should be close enought to the 2D landmarks\n",
    "        -------------------------------------------------------------------\n",
    "        \"\"\"\n",
    "        world_landmarks_list = []\n",
    "\n",
    "        # # Get the camera matrix using the helper function\n",
    "        # camera_matrix = get_camera_matrix(frame.shape[1], frame.shape[0], scale=1.0)\n",
    "\n",
    "        reprojection_points_list = []\n",
    "\n",
    "        # if detection_result:\n",
    "        #     # Apply solvePnP to get world landmarks from image landmarks\n",
    "        #     world_landmarks_list = solvepnp(model_landmarks_list, [detection_result], camera_matrix, frame.shape[1], frame.shape[0])\n",
    "            \n",
    "        #     # Reproject the 3D landmarks back to 2D image\n",
    "        #     reprojection_points_list = reproject(world_landmarks_list, camera_matrix, frame.shape[1], frame.shape[0])\n",
    "\n",
    "\n",
    "        # world_landmarks_list = solvepnp(...)\n",
    "        # reprojection_error, reprojection_points_list = reproject(...)\n",
    "        for hand_landmarks in reprojection_points_list:\n",
    "            for l in hand_landmarks:\n",
    "                cv2.circle(frame, (int(l[0]), int(l[1])), 3, (0, 0, 255), 2)\n",
    "        \n",
    "        # Calculating the FPS\n",
    "        currentTime = time.time()\n",
    "        fps = 1 / (currentTime - previousTime)\n",
    "        previousTime = currentTime\n",
    "        \n",
    "        # Displaying FPS on the image\n",
    "        cv2.putText(frame, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "        # Display the resulting image\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow(\"\", frame)\n",
    "    \n",
    "        # Enter key 'q' to break the loop\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "    \n",
    "    # When all the process is done\n",
    "    # Release the capture and destroy all windows\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The real Deal\n",
    "\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Create a MediaPipe HandLandmarker detector. \n",
    "# Requires MediaPipe 0.9.1 and above.\n",
    "base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
    "options = vision.HandLandmarkerOptions(base_options=base_options, num_hands=1)\n",
    "detector = vision.HandLandmarker.create_from_options(options)\\\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.4, min_tracking_confidence=0.3)\n",
    "\n",
    "def predict(frame):\n",
    "    # Convert the color space from BGR (OpenCV default) to RGB (MediaPipe expects)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    # If hands are detected, return the results\n",
    "    return results\n",
    " \n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(image, detection_result):\n",
    "    \"\"\"\n",
    "    A helper function to draw the detected 2D landmarks on an image.\n",
    "    \"\"\"\n",
    "    # Check if detection_result is None or if no hand landmarks were detected\n",
    "    if detection_result is None or not detection_result.multi_hand_landmarks:\n",
    "        return image\n",
    "    \n",
    "    # Since detection_result.multi_hand_landmarks is not None, proceed to draw landmarks\n",
    "    for hand_landmarks in detection_result.multi_hand_landmarks:\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks.landmark\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style())\n",
    "    return image\n",
    "\n",
    "def get_camera_matrix(frame_width, frame_height, scale=1.0): \n",
    "    \"\"\"\n",
    "    The camera matrix is a matrix of size 3x3 that captures the intrinsic properties of the camera including focal length and center of projection. \n",
    "    One can project a 3D point in the camera space to the image plane by multiplying it with the intrinsic matrix. \n",
    "    \n",
    "    For example, let the 3D point by P = np.array([X, Y, Z]) (column vector). Let camera matrix be K. In numpy's code, the projected point is: \n",
    "    \n",
    "    p = K @ P \n",
    "    p[0] /= p[2]\n",
    "    p[1] /= p[2]\n",
    "    \n",
    "    Here the division by p[2] is the perspective division. After division, p[0] and p[1] are the x and y coordinate of the image pixel. \n",
    "    \"\"\"\n",
    "    \n",
    "    # As we do not know exactly the focal length, we estimate it by a scale of the image size. We can do camera calibration to find a more accurate focal length value but this is out of the scope of this assignment. \n",
    "    focal_length = frame_width * scale \n",
    "    \n",
    "    # Note this aspect ratio reflects ratio in the physical pixel size, almost 1, not the aspect ratio between image width and height as in OpenGL. \n",
    "    aspect_ratio = 1.0\n",
    "    \n",
    "    # Center of projection. We simply take the image center.\n",
    "    center = (frame_width / 2.0, frame_height / 2.0)\n",
    "    \n",
    "    # 3x3 intrinsic matrix\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0, center[0]],\n",
    "        [0, focal_length, center[1]],\n",
    "        [0, 0, 1]], dtype = \"double\"\n",
    "    )\n",
    "    return camera_matrix\n",
    "\n",
    "def get_fov_y(camera_matrix, frame_height):\n",
    "    \"\"\"\n",
    "    Compute the vertical field of view from focal length for OpenGL rendering\n",
    "    \"\"\"\n",
    "    focal_length_y = camera_matrix[1][1]\n",
    "    fov_y = np.rad2deg(2 * np.arctan2(frame_height, 2 * focal_length_y))\n",
    "    return fov_y\n",
    "\n",
    "def get_fov_y3(camera_matrix, frame_height, frame_width):\n",
    "    height = camera_matrix[1, 2] * 2  # Assumes the center_y * 2 is the height\n",
    "    focal_length_y = camera_matrix[1, 1]\n",
    "    fov_y = 2 * np.arctan(height / (2 * focal_length_y))\n",
    "    fov_y_deg = np.degrees(fov_y)\n",
    "    return fov_y_deg\n",
    "\n",
    "def get_matrix44(rvec, tvec):\n",
    "\t\"\"\"\n",
    "\tConvert the rotation vector and translation vector to a 4x4 matrix\n",
    "\t\"\"\"\n",
    "\trvec = np.asarray(rvec)\n",
    "\ttvec = np.asarray(tvec)\n",
    "\tT = np.eye(4)\n",
    "\tR, jac = cv2.Rodrigues(rvec)\n",
    "\tT[:3, :3] = R\n",
    "\tT[:3, 3] = tvec\n",
    "\treturn T\n",
    "\n",
    "def solvepnp(model_landmarks, image_landmarks, camera_matrix, frame_width, frame_height):\n",
    "    \"\"\"\n",
    "    Solve a global rotation and translation to bring the hand model points into the camera space, so that their projected points match the hands.\n",
    "\n",
    "    Input:\n",
    "      model_landmarks: a 21x3 matrix representing hand landmarks. The coordinates are relative to the hand center.\n",
    "\n",
    "      image_landmarks: a 21x2 matrix representing hand landmarks in image space, normalized to [0, 1].\n",
    "\n",
    "    Output:\n",
    "      world_landmarks: a 21x3 matrix representing hand landmarks in absolute world space.\n",
    "    \"\"\"\n",
    "    if len(model_landmarks) == 0 or len(image_landmarks) == 0:\n",
    "        return None\n",
    "\n",
    "    # Prepare input data for solvePnP\n",
    "    model_points = np.float32(model_landmarks)\n",
    "    image_points = np.float32(image_landmarks) * [frame_width, frame_height]\n",
    "\n",
    "    # Camera distortion parameters are assumed to be zero\n",
    "    dist_coeffs = np.zeros((4,1))\n",
    "\n",
    "    # Solve for rotation and translation vectors\n",
    "    success, rotation_vector, translation_vector = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "    # Use cv2.projectPoints to transform model landmarks to image plane\n",
    "    if success:\n",
    "        world_points, _ = cv2.projectPoints(model_points, rotation_vector, translation_vector, camera_matrix, dist_coeffs)\n",
    "        world_landmarks = world_points.reshape(-1, 3)\n",
    "        return world_landmarks\n",
    "\n",
    "    return None\n",
    "\n",
    "def solvepnp2(model_landmarks_list, image_landmarks_list, camera_matrix, dist_coeffs=np.zeros((4, 1))):\n",
    "    if not model_landmarks_list:\n",
    "        return []\n",
    "\n",
    "    world_landmarks_list = []\n",
    "    rvec_list = []\n",
    "    tvec_list = []\n",
    "\n",
    "    for (model_landmarks, image_landmarks) in zip(model_landmarks_list, image_landmarks_list):\n",
    "        object_points = np.array([[landmark.x, landmark.y, landmark.z] for landmark in model_landmarks], dtype='double')\n",
    "        image_points = np.array([[landmark.x, landmark.y] for landmark in image_landmarks], dtype='double')\n",
    "\n",
    "        success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "            object_points, image_points, camera_matrix, dist_coeffs\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            rvec_list.append(rotation_vector)\n",
    "            tvec_list.append(translation_vector)\n",
    "            T = get_matrix44(rotation_vector, translation_vector)\n",
    "            model_points_hom = np.hstack((object_points, np.ones((object_points.shape[0], 1))))\n",
    "            world_points_hom = (T @ model_points_hom.T).T\n",
    "            world_points = world_points_hom[:, :3]\n",
    "            world_landmarks_list.append(world_points)\n",
    "\n",
    "    return world_landmarks_list, rvec_list, tvec_list\n",
    "\n",
    "\n",
    "\n",
    "def reproject(world_landmarks_list, image_landmarks_list, \n",
    "              camera_matrix, frame_width, frame_height): \n",
    "    \"\"\"\n",
    "    Perform a perspective projection of 3D points onto the image plane\n",
    "    and return the projected points.\n",
    "    \"\"\"\n",
    "    reprojection_points_list = []\n",
    "    reprojection_error = 0.0\n",
    "    for (world_landmarks, image_landmarks) in zip(world_landmarks_list, image_landmarks_list):\n",
    "        # Perspective projection by multiplying with the intrinsic matrix\n",
    "        output = world_landmarks.dot(camera_matrix.T)\n",
    "        \n",
    "        # Perspective division\n",
    "        output[:, 0] /= output[:, 2]\n",
    "        output[:, 1] /= output[:, 2]\n",
    "        \n",
    "        # Store the results into a list for visualization later\n",
    "        reprojection_points_list.append(output[:, :2])\n",
    "    \n",
    "        # Calculate the reprojection error, per point\n",
    "        image_points = np.float32([[l.x * frame_width, l.y * frame_height] for l in image_landmarks])\n",
    "        reprojection_error += np.linalg.norm(output[:, :2] - image_points) / len(output) / len(world_landmarks_list)\n",
    "    \n",
    "    return reprojection_error, reprojection_points_list\n",
    "\n",
    "\"\"\"\n",
    "This is an example main function that displays the video camera and the detection results in 2D landmarks with an OpenCV window.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    # (0) in VideoCapture is used to connect to your computer's default camera\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Initializing current time and precious time for calculating the FPS\n",
    "    previousTime = 0\n",
    "    currentTime = 0\n",
    "\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break  # Break if failed to capture frame\n",
    "\n",
    "        # Resizing the frame for better view\n",
    "        aspect_ratio = frame.shape[1] / frame.shape[0]\n",
    "        frame_resized = cv2.resize(frame, (int(720 * aspect_ratio), 720))\n",
    "        # Resize frame for faster processing\n",
    "        # frame_resized = cv2.resize(frame, (640, 480))  # Example resolution, adjust based on your needs\n",
    "\n",
    "\n",
    "        # Convert the frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Making predictions to detect hands and their landmarks\n",
    "        detection_result = predict(frame_rgb)\n",
    "\n",
    "        # Draw 2D landmarks on the image\n",
    "        frame_with_landmarks = draw_landmarks_on_image(np.copy(frame_resized), detection_result)\n",
    "\n",
    "        # Calculate the FPS\n",
    "        currentTime = time.time()\n",
    "        fps = 1 / (currentTime - previousTime)\n",
    "        previousTime = currentTime\n",
    "\n",
    "        # Displaying FPS on the image\n",
    "        cv2.putText(frame_with_landmarks, f'FPS: {int(fps)}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the resulting image\n",
    "        cv2.imshow(\"Frame\", frame_with_landmarks)\n",
    "\n",
    "        # Break the loop on 'q' keypress\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # When everything is done, release the capture and destroy all windows\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
